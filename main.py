import asyncio
import os

import streamlit as st
from mcp.client.streamable_http import streamablehttp_client
from strands import Agent
from strands.models.openai import OpenAIModel
from strands.tools.mcp import MCPClient

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="Microsoft Learning MCP Ã— Strands Agents SDK",
    page_icon="ğŸ“šğŸ¤–",
    initial_sidebar_state="expanded",
    menu_items={"About": "Microsoft Learning MCPã¨Strands Agents SDKã§AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½“é¨“ã§ãã‚‹ã‚¢ãƒ—ãƒªã ã‚ˆï¼"},
)

# ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
if "openai" in st.secrets:
    os.environ["OPENAI_API_KEY"] = st.secrets["openai"]["OPENAI_API_KEY"]

# LangSmithãƒˆãƒ¬ãƒ¼ã‚¹è¨­å®š
if "langsmith" in st.secrets:
    os.environ["LANGSMITH_API_KEY"] = st.secrets["langsmith"]["LANGSMITH_API_KEY"]


def setup_langsmith_tracing(api_key, project_name, enabled=True):
    """LangSmithãƒˆãƒ¬ãƒ¼ã‚¹æ©Ÿèƒ½ã‚’è¨­å®š"""
    if enabled and api_key and project_name:
        os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = "https://api.smith.langchain.com/otel"
        os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"x-api-key={api_key},Langsmith-Project={project_name}"
        os.environ["OTEL_SERVICE_NAME"] = "strands-mcp-agent"
        os.environ["STRANDS_OTEL_SAMPLER_RATIO"] = "0.2"
        return True
    else:
        # ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ç„¡åŠ¹åŒ–
        for env_var in [
            "OTEL_EXPORTER_OTLP_ENDPOINT",
            "OTEL_EXPORTER_OTLP_HEADERS",
            "OTEL_SERVICE_NAME",
            "STRANDS_OTEL_SAMPLER_RATIO",
        ]:
            if env_var in os.environ:
                del os.environ[env_var]
        return False


# Microsoft Learning MCPè¨­å®šï¼ˆå›ºå®šï¼‰
MICROSOFT_LEARNING_MCP_URL = "https://learn.microsoft.com/api/mcp"

# ãƒ¢ãƒ‡ãƒ«é¸æŠè¨­å®š
SUPPORTED_MODELS = ["gpt-4.1", "o3"]
DEFAULT_MODEL_INDEX = 0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯gpt-4.1


def create_mcp_client(mcp_url):
    """MCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆï¼ˆHTTPå½¢å¼ï¼‰"""

    def transport():
        return streamablehttp_client(mcp_url)

    return MCPClient(transport)


def create_agent(clients, model_id="gpt-4.1", messages=None):
    """è¤‡æ•°ã®MCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰ãƒ„ãƒ¼ãƒ«ã‚’é›†ç´„ã—ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆã€‚messagesã§å±¥æ­´ã‚‚æ¸¡ã›ã‚‹"""
    all_tools = []
    for client in clients:
        tools = client.list_tools_sync()
        all_tools.extend(tools)

    model = OpenAIModel(
        client_args={
            "api_key": os.getenv("OPENAI_API_KEY"),
        },
        model_id=model_id,
    )
    if messages is not None:
        return Agent(model=model, tools=all_tools, messages=messages)
    else:
        return Agent(model=model, tools=all_tools)


def extract_tool_info(chunk):
    """ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ãƒ„ãƒ¼ãƒ«æƒ…å ±ã‚’æŠ½å‡º"""
    event = chunk.get("event", {})
    if "contentBlockStart" in event:
        tool_use = event["contentBlockStart"].get("start", {}).get("toolUse", {})
        return tool_use.get("toolUseId"), tool_use.get("name")
    return None, None


def extract_text(chunk):
    """ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    if text := chunk.get("data"):
        return text
    elif delta := chunk.get("delta", {}).get("text"):
        return delta
    return ""


async def stream_response(agent, latest_user_input):
    """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤ºã—ã€å®Œå…¨ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™"""
    text_holder = st.empty()
    buffer = ""
    full_response = ""
    shown_tools = set()

    async for chunk in agent.stream_async(latest_user_input):
        if isinstance(chunk, dict):
            tool_id, tool_name = extract_tool_info(chunk)
            if tool_id and tool_name and tool_id not in shown_tools:
                shown_tools.add(tool_id)
                if buffer:
                    text_holder.markdown(buffer)
                    full_response += buffer
                    buffer = ""
                st.info(f"ğŸ”§ **{tool_name}** ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œä¸­...")
                text_holder = st.empty()

            if text := extract_text(chunk):
                buffer += text
                text_holder.markdown(buffer)

    if buffer:
        full_response += buffer
        text_holder.markdown(buffer)

    return full_response


# LangSmithãƒˆãƒ¬ãƒ¼ã‚¹è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°/ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‹ã‚‰è‡ªå‹•è¨­å®šï¼‰
if "langsmith" in st.secrets:
    setup_langsmith_tracing(st.secrets["langsmith"]["LANGSMITH_API_KEY"], "strands-mcp-agent", True)

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ãƒ¢ãƒ‡ãƒ«é¸æŠ
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    selected_model = st.selectbox(
        "ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„",
        options=SUPPORTED_MODELS,
        index=DEFAULT_MODEL_INDEX,
        help="AIãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã§ãã¾ã™ã€‚o3ã¯æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚"
    )
    st.info(f"é¸æŠä¸­ã®ãƒ¢ãƒ‡ãƒ«: **{selected_model}**")

# --- App ---
st.title("Microsoft Learning Agent")
st.markdown(
    "ã“ã®ã‚¢ãƒ—ãƒªã§ã¯ã€MS Learnã®MCP APIã‚’ä½¿ã£ã¦Azureã®è³‡æ ¼å‹‰å¼·ã‚„å­¦ç¿’ã‚µãƒãƒ¼ãƒˆã‚‚ã§ãã¡ã‚ƒã†ã‚ˆï¼\n"
    "\nğŸ’¡ Azureã®å…¬å¼ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°æ•™æã‚’æ´»ç”¨ã—ã¦ã€è³‡æ ¼å–å¾—ã‚’ç›®æŒ‡ãã†ï¼"
)

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []

# å±¥æ­´ã‹ã‚‰ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # contentã¯å¿…ãšlistå½¢å¼ã§æ ¼ç´ã•ã‚Œã¦ã„ã‚‹å‰æ
        text = "".join([c.get("text", "") for c in message["content"]])
        st.markdown(text)

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’å—ã‘ä»˜ã‘ã‚‹
if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
    if prompt and prompt.strip():
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”»é¢ã«è¡¨ç¤ºï¼ˆã¾ã å±¥æ­´ã«ã¯ä¿å­˜ã—ãªã„ï¼‰
        with st.chat_message("user"):
            st.markdown(prompt)

        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”
        with st.chat_message("assistant"):
            with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­â€¦"):
                client = create_mcp_client(MICROSOFT_LEARNING_MCP_URL)
                clients = [client]
                try:
                    for client in clients:
                        client.__enter__()

                    # éå»ã®å±¥æ­´ã®ã¿ã§Agentã‚’ä½œæˆ
                    agent = create_agent(clients, model_id=selected_model, messages=st.session_state.messages.copy())

                    # æœ€æ–°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ï¼ˆstrï¼‰ã‚’stream_responseã«æ¸¡ã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
                    response = asyncio.run(stream_response(agent, prompt))

                    # AIã®å¿œç­”ãŒå®Œäº†ã—ãŸã‚‰ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã¨AIå¿œç­”ã‚’å±¥æ­´ã«ä¿å­˜
                    st.session_state.messages.append({"role": "user", "content": [{"text": prompt}]})
                    st.session_state.messages.append({"role": "assistant", "content": [{"text": response}]})

                except asyncio.TimeoutError:
                    st.error("ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
                except Exception as e:
                    st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                    st.info("Microsoft Learning MCPã‚µãƒ¼ãƒãƒ¼ã¸ã®æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                finally:
                    for client in clients:
                        try:
                            client.__exit__(None, None, None)
                        except Exception:
                            pass
